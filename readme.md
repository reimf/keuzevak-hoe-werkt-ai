# Readme

Weeks
1. General introduction
   1. Large Language Models like ChatGPT are the talk of the town
   2. Algorithms vs artificial intelligence = instructions vs learning
2. [Neuron & Perceptron](https://colab.research.google.com/github/reimf/keuzevak-hoe-werkt-ai/blob/main/perceptron.ipynb)
   1. Link between brains (neuron) and AI (perceptron) - McCulloch & Pitts (1943)
   2. Supervised learning, lineair classifier, delta rule
   3. Show OR, AND & XOR (doesn't work) - Minsky & Papert (1969)
   4. Not much till 1980
3. Multilayered perceptrons
   1. Show XOR with preset weights
   2. No training algorithm for multilayered perceptrons with the heaviside step-function exists
   3. Sigmoid, backpropagation, gradient descent
   4. XOR with MLP
4. Optical Character Recognition with ANN
   1. MNIST digits
   2. Preprocessing the data, postprocessing the prediction
   3. Training set, test set, validation set, local minima
5. Language model?
6. The truth is in the data
   1. Different models lead to the same results
   2. Is the data representative of real-world situations? Valid? Legal? Non-discriminatory?
7. The current state of AI
   1. Applied in many niches
   2. General AI: generate text (ChatGPT), images (DALL-E), music (Udio), speech-to-text, text-to-speech, etc

Further material:
- https://dev.to/trekhleb/interactive-machine-learning-experiments-3ga7
- https://www.kaggle.com/code/prashant111/comprehensive-guide-to-ann-with-keras
- https://www.geeksforgeeks.org/implementation-of-perceptron-algorithm-for-xor-logic-gate-with-2-bit-binary-input/